Clinical Diagnosis Needs More Than One Mind: Multi-View Diagnostic Reasoning in Large Language Models
---

Clinical diagnosis requires integrating multiple complementary reasoning strategies. This project explores two approaches to enable LLMs to â€œreason with more than one mindâ€:

- **Collaborative Multi-Agent Diagnostic Reasoning**
- **Internalized Multi-View Reasoning**

## ğŸ” Overview

## ğŸ›  Installation

```bash
# Clone the repository
git clone https://github.com/JaneLeeAug/Multi-view-diagnostic-reasoning-in-LLMs.git
cd Multi-view-diagnostic-reasoning-in-LLMs

# Install dependencies
pip install -r requirements.txt
```
## ğŸ“ Repository Structure

Multi-view-diagnostic-reasoning-in-LLMs/
â”œâ”€â”€ Collaborative Multi-Agent Reasoning/
â”‚   â”œâ”€â”€ GPT-4_multi-agent.xlsx
â”‚   â”œâ”€â”€ GPT-4o_multi-agent.xlsx
â”‚   â”œâ”€â”€ multi-agent_round1.ipynb
â”‚   â””â”€â”€ multi-agent_round2.ipynb
â”œâ”€â”€ Internalized Multi-View Reasoning/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ gpt-4o_training_data.xlsx
â”‚   â”‚   â”œâ”€â”€ gpt-oss-120b_training_data.xlsx
â”‚   â”‚   â””â”€â”€ results.xlsx
â”‚   â””â”€â”€ script/
â”‚       â”œâ”€â”€ gpt-oss-20b.py
â”‚       â”œâ”€â”€ gpt-oss-120b.py
â”‚       â”œâ”€â”€ llama-3.1_8b.ipynb
â”‚       â”œâ”€â”€ mistral-7b-instruct-v0.3.ipynb
â”‚       â”œâ”€â”€ phi-4-mini-instruct.ipynb
â”‚       â””â”€â”€ gpt-oss-120b_API.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

## ğŸš€ Usage

### Collaborative Multi-Agent Diagnostic Reasoning

To run **Collaborative Multi-Agent Reasoning (CMAR)** and **Independent Multi-Agent Reasoning (IMAR)**:

1. Execute `multi-agent_round1.ipynb` to run IMAR.
2. Execute `multi-agent_round2.ipynb` to run CMAR, which builds upon IMAR results.

This two-step process ensures that agents first reason independently and then collaborate to refine their diagnosis.

### Internalized Multi-View Reasoning

1. The teacher model generates training data using **`multi-agent_round1.ipynb`** and **`gpt-oss-120b_API.ipynb`**, saved as:

   - `gpt-4o_training_data.xlsx`
   - `gpt-oss-120b_training_data.xlsx`

   Either dataset can be converted into a CSV file for training.

2. The dataset used to evaluate the student model is **`testing_data.csv`**.

3. Run the corresponding script or Colab notebook depending on the student model:

   | Student Model                   | Script / Notebook |
   |---------------------------------|-------------------|
   | **gpt-oss-20b**                 | `gpt-oss-20b.py` |
   | **gpt-oss-120b**                | `gpt-oss-120b.py` |
   | **LLaMA 3.1 8B**                | [Open Notebook](https://colab.research.google.com/drive/1G2wBf3C9V4Ita5O1TZLKn9JxyVFkWzPd?usp=sharing) |
   | **Mistral 7B Instruct v0.3**    | [Open Notebook](https://colab.research.google.com/drive/1Uz6vhClCYjFxn5h-aj9cua7wJ96Sc32L?usp=sharing) |
   | **Phi-4 Mini Instruct**         | [Open Notebook](https://colab.research.google.com/drive/1AhWKg44x_1Ssmpn655V3SU5cKLYteRBI?usp=sharing) |

4. The responses of both raw and fine-tuned student models are summarized in **`results.xlsx`**.
